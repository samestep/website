<!doctype html>
<html lang="en-us">
  <head>
    <meta charset="utf-8" />
    <link rel="apple-touch-icon" href="/icon.png" />
    <link rel="icon" type="image/png" href="/icon.png" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.10.0/styles/monokai.min.css"
    />
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css"
    />
    <link rel="stylesheet" href="/blog.css" />
    <title>Parallelizing nvcc | Sam Estep</title>
  </head>
  <body>
    <main>
      <h1>Parallelizing nvcc</h1>
      <p>
        <em>by <a href="/">Sam Estep</a>, 2021-02-20</em>
      </p>
      <div>
        <p>
          <em>Edit (2021-04-29):</em> According to the
          <a href="https://stackoverflow.com/a/51115889">Stack Overflow</a>:
        </p>
        <blockquote>
          <p>
            <strong
              ><code>nvcc</code> from CUDA 11.3 finally supports this out of the
              box via the
              <a
                href="https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#options-for-guiding-compiler-driver-threads"
                ><code>-t</code> flag</a
              >.</strong
            >
          </p>
        </blockquote>
        <p>CUDA 11.3 was released on 2021-04-15.</p>
        <hr />
        <p>
          This post talks about
          <a
            href="https://github.com/pytorch/pytorch/blob/5adbace8e6c3b425c3f4805eb9d12e728708bd33/tools/fast_nvcc/fast_nvcc.py"
            ><code>fast_nvcc.py</code></a
          >, which <a href="https://github.com/malfet">Nikita Shulga</a>,
          <a href="https://github.com/walterddr">Rong Rong</a>, and I brought to
          the <a href="https://github.com/pytorch/pytorch">PyTorch repo</a> in
          December, reducing wall-clock build time for builds that use
          <code>nvcc</code> to compile large CUDA files for multiple
          architectures at once. I first give the motivation and a broad
          overview of the approach, and then explore several technical details
          of the development process.
        </p>
        <h2>Overview</h2>
        <p>
          As
          <a
            href="https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html"
            >documented by NVIDIA</a
          >, <code>nvcc</code> is a &quot;compiler driver&quot; which manages
          the &quot;several splitting, compilation, preprocessing, and merging
          steps for each CUDA source file.&quot; It parses CLI arguments,
          creates a plan for which tools to invoke to do its job (and the CLI
          arguments to pass to those tools), and then executes that plan.
        </p>
        <p>
          The key to our story here is that we can pass <code>nvcc</code> the
          <a
            href="https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#options-for-guiding-compiler-driver-dryrun"
            ><code>--dryrun</code></a
          >
          argument to make it just tell us the plan, rather than actually
          executing it. For instance, let's say we want to compile this simple
          CUDA file:
        </p>
        <pre><code class="language-cuda">#include &lt;stdio.h&gt;

__global__ void kernel() {
  printf(&quot;Hello World of CUDA\n&quot;);
}

int main() {
  kernel&lt;&lt;&lt;1,1&gt;&gt;&gt;();
  return cudaDeviceSynchronize();
}
</code></pre>
        <p>
          We can run the following to make <code>nvcc</code> tell us what it
          <em>would</em> do to compile <code>hello.cu</code> for architectures
          <code>30</code>, <code>50</code>, <code>60</code>, and
          <code>70</code> (and produce an executable called <code>hello</code>):
        </p>
        <pre><code>$ NVCC_ARGS='hello.cu -gencode arch=compute_30,code=sm_30 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_70,code=sm_70 -o hello'
$ nvcc --dryrun $NVCC_ARGS
</code></pre>
        <p>
          This command runs almost instantly, produces no files, and prints to
          stderr something similar to <a href="dryrun.txt">this</a>, listing all
          the commands <code>nvcc</code> would run and the environment variables
          it would set while running them.
        </p>
        <p>
          So at this point we can reproduce the behavior of nvcc by running it
          with <code>--dryrun</code>, parsing the output, setting those
          environment variables, and running those commands. If we do that, we
          find that a significant portion of the total compilation time (see the
          &quot;Flags&quot; section below for numbers) is taken by the
          <code>gcc</code>, <code>cicc</code>, and <code>ptxas</code> commands,
          of which there is one of each, <em>for each architecture</em> (plus
          three extra <code>gcc</code> commands to do other things). These
          architecture-specific steps are essentially independent of each other,
          but <code>nvcc</code> runs them all sequentially, missing out on
          concurrency possibilities.
        </p>
        <p>
          That's where <code>fast_nvcc.py</code> comes in: it serves as a
          (mostly) transparent drop-in replacement for <code>nvcc</code>, which
          works by taking the output from <code>nvcc --dryrun</code> and running
          commands concurrently whenever possible (by generating a dataflow
          dependency graph from the <code>/tmp/tmpxft*</code> filenames in the
          commands). If you have it on your <code>PATH</code>, you should be
          able to replace <code>nvcc ...</code> with
          <code>fast_nvcc.py -- ...</code> anywhere (with some exceptions; see
          the &quot;Limitations&quot; section below).
        </p>
        <h2>Implementation</h2>
        <p>
          This section discusses several (hopefully interesting?) details about
          the <code>fast_nvcc.py</code> implementation and development process.
        </p>
        <h3>Flags</h3>
        <p>
          Going along with the theme of working as a standalone script,
          <code>fast_nvcc.py</code> provides several CLI flags that I found
          invaluable while developing this diff. (If you have it on your
          <code>PATH</code>, you can see them by running
          <code>fast_nvcc.py --help</code>.) I left these in the final product
          in case someone finds them useful in the future.
        </p>
        <pre><code>$ mkdir files
$ fast_nvcc.py --graph=graph.dot --save=files --table=table.csv --verbose=verbose.txt -- $NVCC_ARGS
$ dot -Tpng graph.dot -o graph.png
</code></pre>
        <p>
          This populates <a href="files.txt"><code>files</code></a> and
          generates <a href="graph.dot"><code>graph.dot</code></a
          >, <a href="table.csv"><code>table.csv</code></a
          >, and <a href="verbose.txt"><code>verbose.txt</code></a
          >. And if you have <code>dot</code> installed, that last command
          generates <code>graph.png</code> which looks like this:
        </p>
        <p><img src="graph.png" alt="The generated graph.png file." /></p>
        <p>A few notes:</p>
        <ul>
          <li>
            The <code>--verbose</code> flag differs from <code>nvcc</code>'s
            built-in <code>--dryrun</code> and <code>--verbose</code>
            flags because it shows the (modified) commands that will be run by
            <code>fast_nvcc.py</code> (rather than the originals from
            <code>nvcc</code>), and because it tries to make the commands more
            readable by numbering them and spreading each one across multiple
            lines using Python's
            <a href="https://docs.python.org/3/library/shlex.html#shlex.split"
              ><code>shlex.split</code></a
            >
            function. It's not very friendly for machine consumption, but it's
            useful for humans, and provides context for the other flags since
            the numbers are the same as those shown in the output of
            <code>--graph</code>, for instance.
          </li>
          <li>
            The arrows in the <code>--graph</code> point from dependencies to
            the commands that depend on them, so essentially, the graph shown in
            the picture above gets run from top to bottom.
          </li>
          <li>
            The <code>--table</code> flag was added before <code>--save</code>,
            so all its columns past the first two are a bit redundant (they just
            give the sizes of files, while <code>--save</code> gives the files
            themselves). Its second column was useful for finding what to
            parallelize and what to ignore: see the &quot;Leftovers&quot;
            section below.
          </li>
        </ul>
        <p>
          Here's an Excel chart generated from the first two columns of the
          <code>table.csv</code> file. This shows us that we are indeed
          parallelizing the most time-consuming parts of the compilation:
        </p>
        <p>
          <img
            src="table.png"
            alt="An Excel chart generated from the first two columns of the table.csv
file."
          />
        </p>
        <h3>Tweaks</h3>
        <p>
          While the high-level description in the &quot;Approach&quot; section
          above is mostly accurate, there were a few tweaks that we needed to
          make to get it to actually work. To clarify, the <em>general</em> rule
          is that if two commands refer to the same
          <code>/tmp/tmpxft*</code> filename, then we make the latter wait for
          the former before running. We had a bug at first because some of the
          filenames leave off the <code>/tmp</code> at the beginning (i.e.
          they're just relative paths starting with <code>tmpxft</code>) but it
          mostly works.
        </p>
        <p>
          The first exception we found was for <code>*.fatbin.c</code> files. As
          mentioned in a comment in the <code>fast_nvcc.py</code> source code,
          the <code>cicc</code> commands actually refer to such files (which,
          incidentally, are the ones that don't start with <code>/tmp</code>)
          <em>before</em> the <code>fatbinary</code> command that generates
          them. This is because the <code>cicc</code> command simply generates a
          file that references (via <code>#include</code>) the
          <code>*.fatbin.c</code> file which will be created later, and then
          when the file generated by <code>cicc</code> is processed later, that
          is when the <code>*.fatbin.c</code> file needs to exist. So in this
          case, the rule is: if commands <em>A</em> and <em>B</em> (of which
          <em>B</em> is a fatbinary command) both reference the same
          <code>*.fatbin.c</code> file, and <em>A</em> also references a
          (different) file that is later referenced by command <em>C</em>, then
          <em>C</em> depends on <em>B</em>.
        </p>
        <p>
          Another exception was for the <code>*.module_id</code> files used by
          <code>cicc</code> and <code>cudafe++</code>
          commands. This one led me down a very deep rabbit hole. The issue here
          is that several commands (including one for each architecture)
          reference the same
          <code>*.module_id</code> file, with the first
          <code>cicc</code> command being passed the
          <code>--gen_module_id_file</code> flag to generate the file that is
          then used (without modifying) by the other commands.
        </p>
        <ul>
          <li>
            Running them in all parallel (even after waiting for the first to
            finish) doesn't immediately work, because even though none of them
            modify the file, for some reason they conflict when they access it
            at the same time.
          </li>
          <li>
            An easy &quot;solution&quot; would be to just run the first command,
            copy the one file into a bunch of different files (so they have
            different names) and then run each of the other commands with its
            own unique <code>*.module_id</code> filename, but that's suboptimal
            because it means that you have to run the first two
            <code>cicc</code> commands (which, remember, are some of the most
            expensive in <code>nvcc</code> runs) serially.
          </li>
          <li>
            At first we tried just giving each command its own unique
            <code>*.module_id</code> filename <em>and</em> the
            <code>--gen_module_id_file</code> flag, so they would each generate
            their own file and work completely independently. This actually
            works in most cases! All the <code>*.module_id</code> files get
            slightly different contents this way, since they're all generated by
            different commands, but usually that doesn't matter. However, it
            fails for
            <a
              href="https://github.com/pytorch/pytorch/blob/32b098baf936b63ee23017f6bba4f3e4c56f22a6/aten/src/ATen/native/cuda/SoftMax.cu"
              ><code>aten/src/ATen/native/cuda/SoftMax.cu</code></a
            >, which does a special thing that most CUDA files (at least in
            PyTorch) don't do.
          </li>
        </ul>
        <p>
          So in the end we just looked at the contents of some
          <code>*.module_id</code> files (they're not very large), took a guess
          at the function used to generate them, and replaced the usage of the
          <code>--gen_module_id_file</code> flag with a bunch of
          <code>echo</code> commands that each create a
          <code>*.module_id</code> file with contents similar to what would have
          been created by the first cicc command. We know that the last 8
          characters of the file contents are definitely wrong: they appear to
          probably be a hash of some sort (since they don't (usually) change
          when compiling the same file multiple times), but it's unclear what
          hash function is being used or what exactly is being fed to that hash
          function. So we just take an MD5 checksum (although it would almost
          certainly be better to use a cheaper hash function) of the preceding
          part of the <code>*.module_id</code> contents, stick it on the end,
          and call it a day.
        </p>
        <p>
          (Fun detour: you can disable these
          <code>*.module_id</code> shenanigans via the
          <code>--faithful</code> flag, and by combining that with the
          <code>--graph</code> flag, you can see the less-parallel execution
          graph that results!)
        </p>
        <pre><code>$ fast_nvcc.py --faithful --graph=faithful.dot -- $NVCC_ARGS
$ dot -Tpng faithful.dot -o faithful.png
</code></pre>
        <p><img src="faithful.png" alt="The generated faithful.png file." /></p>
        <h3>Leftovers</h3>
        <p>
          As is plainly visible from the above graphs, this diff doesn't
          parallelize <em>all</em> the per-architecture commands run by
          <code>nvcc</code>. Specifically, all the <code>nvlink</code> steps
          still run serially. But as the <code>--table</code> data show, this
          doesn't matter, since <code>nvlink</code> is basically the fastest
          part of the whole compilation. Interestingly, though, architecture
          <code>72</code> is an exception to this rule:
        </p>
        <pre><code>$ fast_nvcc.py --table=/dev/stdout -- hello.cu -gencode arch=compute_70,code=sm_70 -gencode arch=compute_72,code=sm_72 -o hello | grep nvlink | cut -d, -f1-2
14 nvlink,0.007828212808817625
15 nvlink,0.7020692219957709
</code></pre>
        <p>
          As suggested above, <code>nvlink</code> for architecture
          <code>72</code> is two orders of magnitude slower than for any of the
          other architectures, and by far the slowest step for any compilation
          that includes it. Currently it seems to be the only such anomalously
          slow architecture, but if more are introduced in the future, the
          sequentiality of the <code>nvlink</code> steps in
          <code>fast_nvcc.py</code> may become important. It's unclear whether
          it's possible to parallelize those steps, though.
        </p>
        <h3>Limitations</h3>
        <p>
          Since <code>fast_nvcc.py</code> depends on being able to infer
          dataflow dependencies using <code>/tmp/tmpxft*</code> filenames, it
          assumes the <code>/tmp</code> prefix. According to
          <a
            href="https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#options-for-guiding-compiler-driver"
            >the <code>nvcc</code> docs</a
          >, there are a few ways that this assumption could be violated:
        </p>
        <ul>
          <li>if <code>TMPDIR</code> is set, its value is used instead</li>
          <li>
            if the OS is Windows, <code>C:\Windows\temp</code> (or the value of
            <code>TEMP</code>) is used instead
          </li>
          <li>
            if <code>--objdir-as-tempdir</code> is passed, the directory of the
            object file is used instead
          </li>
          <li>
            if <code>--keep</code> or <code>--save-temps</code> is passed, the
            current directory is used instead
          </li>
          <li>
            if <code>--keep-dir</code> is passed, its value is used instead
          </li>
        </ul>
        <p>
          This limitation could be removed, but we haven't need to address it
          yet, so instead, <code>fast_nvcc.py</code> simply warns the user
          (linking to the <code>nvcc</code> docs) if any of the above conditions
          occur. As an aside, it doesn't appear immediately obvious what the
          precedence of these rules would be; perhaps
          <code>fast_nvcc.py</code> should still warn if multiple of them occur
          simultaneously.
        </p>
        <h2>Recap</h2>
        <p>
          We looked at <code>fast_nvcc.py</code>---which acts as a faster (wall
          time) replacement for <code>nvcc</code> when used to compile CUDA
          files for multiple architectures at once---and explored several of its
          implementation details. Thanks for reading! :)
        </p>
      </div>
    </main>
  </body>
</html>
